{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from glob import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nb\n",
    "from nilearn import image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_code.data_io import Dataset_Pipeline, _get_data\n",
    "\n",
    "devices = ['/gpu:0', '/gpu:1']\n",
    "\n",
    "class_type=True #true if QC, false if site\n",
    "\n",
    "if class_type:\n",
    "    import source_code.models.basic_qc_cnn as model\n",
    "    train_cache_prefix=\"/home/smantra/finalproject/cache_train_qc/\"\n",
    "    eval_cache_prefix=\"/home/smantra/finalproject/cache_eval_qc/\"\n",
    "    d = devices[0]\n",
    "else:\n",
    "    import source_code.models.basic_site_cnn as model\n",
    "    train_cache_prefix=\"/home/smantra/finalproject/cache_train_sites/\"\n",
    "    eval_cache_prefix=\"/home/smantra/finalproject/cache_eval_sites/\"\n",
    "    d = devices[1]\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     log_dir = \"logs\"\n",
    "#     current_run_subdir = os.path.join(\n",
    "#         \"run_\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "#    model_dir = os.path.join(log_dir, model.name, \"106x128x110\")#current_run_subdir)\n",
    "#     model_dir = os.path.join(log_dir, model.name, '592018')\n",
    "\n",
    "#     run_config = tf.estimator.RunConfig(model_dir=model_dir)\n",
    "\n",
    "#     params = tf.contrib.training.HParams(\n",
    "#         target_shape=(106, 128, 110),\n",
    "#         model_dir=model_dir\n",
    "#     )\n",
    "\n",
    "    ds = Dataset_Pipeline(target_shape=(106, 128, 110),\n",
    "                          n_epochs=10,\n",
    "                          train_src_folder=\"/home/smantra/finalproject/data/\",\n",
    "                          train_cache_prefix=\"/home/smantra/finalproject/cache_train/\",\n",
    "                          eval_src_folder=\"/home/smantra/finalproject/eval/\",\n",
    "                          eval_cache_prefix=\"/home/smantra/finalproject/cache_eval/\",\n",
    "                          batch_size=4\n",
    "                         )\n",
    "\n",
    "    # Workaround for cache iterator concurency issues. Iterate over the whole\n",
    "    # training dataset without counterbalancing to make sure everything is\n",
    "    # preprocessed and cached\n",
    "    if not os.path.exists(ds.train_cache_prefix + \".index\"):\n",
    "        print(\"Preprocessing the training set\")\n",
    "        with tf.Session() as sess:\n",
    "            train_dataset = _get_data(batch_size=ds.batch_size,\n",
    "                                      src_folder=ds.train_src_folder,\n",
    "                                      n_epochs=1,\n",
    "                                      cache_prefix=ds.train_cache_prefix,\n",
    "                                      shuffle=False,\n",
    "                                      target_shape=ds.target_shape,\n",
    "                                     )\n",
    "\n",
    "            train_dataset = train_dataset.make_one_shot_iterator()\n",
    "            while True:\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        features, (qc_labels, site_labels) = sess.run(train_dataset.get_next())\n",
    "                        print(qc_labels)\n",
    "                        print(site_labels)\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        print(\"Finished preprocessing the training set\")\n",
    "\n",
    "#     train_spec = tf.estimator.TrainSpec(input_fn=ds.train_input_fn,\n",
    "#                                         max_steps=20000,\n",
    "#                                        )\n",
    "#     eval_spec = tf.estimator.EvalSpec(input_fn=ds.eval_input_fn,\n",
    "#                                       steps=None,\n",
    "#                                       start_delay_secs=0,\n",
    "#                                       throttle_secs=600)\n",
    "\n",
    "#     estimator = tf.estimator.Estimator(model_fn=model.model_fn,\n",
    "#                                        params=params,\n",
    "#                                        config=run_config)\n",
    "    \n",
    "#     config = tf.ConfigProto() \n",
    "#     config.gpu_options.allow_growth = True \n",
    "#     with tf.Session(config=config) as sess:\n",
    "#         with tf.device(d):\n",
    "#             sess.run(tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
